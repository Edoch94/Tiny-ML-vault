Consider map **IMAGE_0**

- Naive approach
	1. Create grid **IMAGE_1**, and focus a specific cell **IMAGE_2_1** **IMAGE_2_2** -> Images looking at different directions would be part of the same class **IMAGE_3** -> may confuse model 
	3. Consider direction **IMAGE_4**
	However: images that are very near may end up in different classes because of quantization approximations
- CosPlace Groups: 
	Divide the classes in groups: adjacent classes are not considered at the same time during training
	1. Minimum separation with respect to spatial location
	2. Minimum separation with respect to orientation


The first step to treat this problem as a classification is to split the dataset into categories, based on the UTM coordinates (north and east labels) and on the orientation (the heading label)
In this way the authors created multiple sets of images, where each set is associated to a class. Here you can see the mathemaical formula for assigning an image (x) to a class
However there is a limitation using this naive approach: almost identical images may end up in different classes, due to quantization approximations, and this might confuse the model.

We prepared this example to show you this first step:
let's consider some images taken near in the area Politecnico
- the first step is divide the images in classes simply based on the position from which they were taken (this is like applying a grid on the map). Now let's look at what happens in a specific cell of the grid
- Here are three images: two of them portray Politecnico's main entrance, the third one instead looks in he opposite way, towards Corso Duca and Monumento "Il Fante", covered by the trees in this photo. These images were taken almost from the same location, but it makes sense to divide them in different classes based on the orientation, otherwise the classifier might get confused. 
- This is the reason why classes consider both the spatial position and the orientation of the images
But let's consider also these other situations
- these two images look at the same direction, but because of the spatial discretization, they end up in two different classes. Something similar can be said in this case: this two images were taken almost in the same position, but because of the angular discretization they end up in different classes. These cases may confuse again the model, because very similar images are associated to different classes

This is way the authors designed the CosPlace Groups, which are set of not adjacent classes, generated by setting a minimum separation between the classe both from the translation and the orientation perspectives. 
Applying this grouping, going back to the previous example, the similar classes are never going to be part of the same CosPlace Group. 
All in all, two classes can be part of the same group only if they are not near two each other or if they do not have a similar orientation

At this point CosPlace method is used to train the model, consisting into sequentially applying the Large Margin Cosine Loss over the CosPlace Groups
Notice that at train time the model learns how to extract descriptors from images, helped by this classification method, but then at test time the LMCL module is discarded, and the model performs a classic retrieval task
This method allow to avoid mining or caching, and is architecture-agnostic, meaning that different backbones perform equally well
In this brief example we show you how it works: the model is applied on one CosPlace Group, represented with the blue color, and after one iteration (composed of multiple epochs), the LMCL module is discarded, a new one is plugged in, and the training goes on, over another CosPlace Group, and so on for multiple steps

After the training, the LCML module is removed, and the model can be used for retrieving from a database images that are similar to a query