# Slides
Consider map **IMAGE_0**

- Naive approach
	1. Create grid **IMAGE_1**, and focus a specific cell **IMAGE_2_1** **IMAGE_2_2** -> Images looking at different directions would be part of the same class **IMAGE_3** -> may confuse model 
	3. Consider direction **IMAGE_4**
	However: images that are very near may end up in different classes because of quantization approximations
- CosPlace Groups: 
	Divide the classes in groups: adjacent classes are not considered at the same time during training
	1. Minimum separation with respect to spatial location
	2. Minimum separation with respect to orientation

# Storyboard

The first step to treat this problem as a classification task is to split the dataset into categories, based on the UTM coordinates (north and east labels) and on the orientation (the heading label)
In this way the authors assigned each image (x in the formula) to a class.
However there is a limitation using this naive approach: almost identical images may end up in different classes, due to quantization approximations, and this might confuse the model.

---

For example, let's consider some images taken in the Politecnico area 

---

- the first step is to divide the images into classes simply based on their positions (this is like applying a grid on the map).

---

Now let's look at what happens in a specific cell of the grid

---

- Here are three images: two of them portray Politecnico's main entrance, while the third one looks in the opposite way, towards Corso Duca. These images were taken almost from the same location, 

- but it makes sense to divide them in different classes based on the orientation, otherwise the classifier may get confused. 
-> This is the reason why classes consider both the spatial position and the orientation of the images

---

But let's consider also these other situations
- these two images look at the same direction, but because of the spatial discretization, they end up in two different classes. 

---

- Something similar can be said in this case: this two images were taken almost from the same position, but because of the angular discretization they end up in different classes. 
These cases may again confuse the model, because very similar images get associated to different classes

---

This is why the authors designed the CosPlace Groups, which are sets of not adjacent classes, generated by setting a minimum separation between the classes both from the translation and the orientation perspectives. 

---

Applying this grouping to the previous examples, the similar classes are never going to be part of the same CosPlace Group. 

---

All in all, two classes can be part of the same CosPlace Group only if they are not near to each other or if they do not have a similar orientation

---

At this point, the CosPlace method is used to train the model, consisting into sequentially applying the Large Margin Cosine Loss over the CosPlace Groups.

Notice that at train time the model learns how to extract meaningful descriptors from images, helped by this classification method, but then at test time the LMCL module is discarded, and the model performs a classic retrieval task.

This method allows to avoid mining or caching, and it is architecture-agnostic, meaning that different backbones perform equally well.

---

In this brief example we show how training and testing work: the model is applied on one CosPlace Group, represented with the blue color, and after one iteration 

---

(composed of multiple epochs), the LMCL module is discarded, a new one is plugged in, and the training goes on, over another CosPlace Group, and so on for multiple steps

---

After the training, the LMCL module is removed, and the model can be used for retrieving from a database the images that are similar to the query, like in a normal retrieval task.