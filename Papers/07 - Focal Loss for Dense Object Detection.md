#paper

# Abstract
The highest accuracy object detectors to date are based on a <u>two-stage approach popularized by R-CNN</u>, where a classifier is applied to a sparse set of candidate object locations. In contrast, **one-stage detectors** that are <u>applied over a regular, dense sampling of possible object locations</u> have the **potential to be faster and simpler**, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the <u>extreme foreground-background class imbalance</u> encountered during <u>training of dense detectors</u> is the central cause. We propose to address this class imbalance by **reshaping the standard cross entropy loss** such that it **down-weights the loss assigned to well-classified examples**. Our novel **Focal Loss** <u>focuses training on a sparse set of hard examples</u> and <u>prevents the vast number of easy negatives from overwhelming the detector</u> during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call **RetinaNet**. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at:
https://github.com/facebookresearch/Detectron.

---

# Introduction
Current state-of-the-art object detectors are based on a two-stage, proposal-driven mechanism. As popularized in the [[R-CNN]] framework, the first stage generates a sparse set of candidate object locations and the second stage classifies each candidate location as one of the foreground classes or as background using a convolutional neural network. Through a sequence of advances, this two-stage framework consistently achieves top accuracy on the challenging COCO benchmark. 

Despite the success of two-stage detectors, a natural question to ask is: could a simple one-stage detector achieve similar accuracy? One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios. Recent work on one-stage detectors, such as [[YOLO]] and [[SSD]], demonstrates promising results, yielding faster detectors with accuracy within 10-40% relative to state-of-the-art two-stage methods. 

This paper pushes the envelop further: we present a one-stage object detector that, for the first time, matches the state-of-the-art COCO AP of more complex two-stage detectors, such as the [[Feature Pyramid Network|Feature Pyramid Network (FPN)]] or [[Mask Regional CNN|Mask R-CNN]] variants of [[Faster R-CNN]]. To achieve this result, we identify class imbalance during training as the main obstacle impeding one-stage detector from achieving state-of-the-art accuracy and propose a new loss function that eliminates this barrier.

Class imbalance is addressed in R-CNN-like detectors by a two-stage cascade and sampling heuristics. The proposal stage (e.g., Selective Search, EdgeBoxes,
DeepMask, RPN) rapidly narrows down the number of candidate object locations to a small number (e.g., 1-2k), filtering out most background samples. In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM), are performed to maintain a manageable balance between foreground and background. 

In contrast, a one-stage detector must process a much larger set of candidate object locations regularly sampled across an image. In practice this often amounts to enumerating âˆ¼100k locations that densely cover spatial positions, scales, and aspect ratios. While similar sampling heuristics may also be applied, they are inefficient as the training procedure is still dominated by easily classified background examples. This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping or hard example mining.

In this paper, we propose a new loss function that acts as a more effective alternative to previous approaches for dealing with class imbalance. The loss function is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases, see Figure 1. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples. Experiments show that our proposed Focal Loss enables us to train a high-accuracy, one-stage detector that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous state-of-the-art techniques for training one-stage detectors. Finally, we note that the exact form of the focal loss is not crucial, and we show other instantiations can achieve similar results.

To demonstrate the effectiveness of the proposed focal loss, we design a simple one-stage object detector called RetinaNet, named for its dense sampling of object locations in an input image. Its design features an efficient in-network feature pyramid and use of anchor boxes. RetinaNet is efficient and accurate; our best model, based on a ResNet-101-FPN backbone, achieves a COCO test-dev AP of 39.1 while running at 5 fps, surpassing the previously best published single-model results from both one and two-stage detectors, see Figure 2.